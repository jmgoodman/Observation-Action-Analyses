{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff5a013",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee957798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "init=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b3464",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0833f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not init:\n",
    "    os.chdir('..')\n",
    "    init=True\n",
    "from pythonfigures.datapartition import DataPartitioner\n",
    "from pythonfigures.neuraldatabase import Query\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.linalg import subspace_angles, null_space, orth\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5647b7c",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPartitioner(session='Moe46',\n",
    "                    areas=['AIP'],\n",
    "                    aligns=['fixation','cue onset','go cue onset','movement onset','hold onset'],\n",
    "                    contexts=['active','passive','control'],\n",
    "                    groupings=['context','alignment','grip','object','turntable','time'])\n",
    "\n",
    "# TODO: [x] try go cue onset as the alignment for the \"visual\" space (i.e., include memory signals, *really* make sure you're isolating those transients!) ([x] also include context & turntable as separable factors in the space you try to remove, and use a 95% variance criterion like the one used in the paper instead of these weak classification criteria)\n",
    "# uh oh! using go cue onset actually preferentially affects VGG performance, with little effect on Obs! All while doubling the number of axes we remove!\n",
    "# moreover, these measures either have a UNIFORM or cross-context effect or no effect at all (regardless, still no preferential reduction in passive classification accuracy) when considering just the mixed turntable\n",
    "# and we get better results for the special turntable control, too (although just barely... it still doesn't jibe with intuition in that there's still substantial classification accuracy even after removing the big fat premovement space)\n",
    "# in any case the cross-decoding controls are unaffected regardless\n",
    "# THEREFORE: just use the visual subspace control\n",
    "# (note: all the above is based on AIP of Moe46, Moe50, Zara70, F5 of Zara70)\n",
    "# (it's also true for the main result of F5 in Moe46, Moe 50, but with selective, albeit *very* slight, reduction in passive performance on the mixed turntable)\n",
    "# (note: these measures actually have the desired effect in AIP, F5 of Zara64)\n",
    "# (Zara68 doesn't have enough neurons...)\n",
    "\n",
    "# TODO: (but: we have one last trick up our sleeve. remove the space that interferes the least with the target decoding space!)\n",
    "# except wait... this ain't gonna work... unless we bake in the assumption that all we care about is the active context, which feels pretty funny\n",
    "# plus we need to either develop a new method or set some kind of arbitrary threshold on the eigenvalues of the difference-of-covariances problem... yeah okay screw it)\n",
    "\n",
    "# TODO: okay the REAL last stand: preferentially preserve time-varying over static components\n",
    "# use the pandas \"transform\" method!\n",
    "# nope! this also doesn't appreciably change the results from the visual case! It preserves more movement-period information but also doesn't detract from encoding during the passive context...\n",
    "# stop getting fancy. just use the visual subspace trick, show you still get decent accuracy, but ***report on the subspace alignment*** and ***subsampling results*** afterward!\n",
    "\n",
    "# (not now) TODO: visualize the time-varying nature of the signals before & after removal... *somehow* (save this for after NCM)\n",
    "\n",
    "# TODO: [x] active vs. passive cross-classification of OBJECT, before & after subspace removal (at least this tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b3b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the whole damn thing\n",
    "df = dp.readQuery(0)\n",
    "\n",
    "# convert from turntable x object ID vs. just the turntable ID\n",
    "df['turntable'] = df['turntable'] // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030906b1",
   "metadata": {},
   "source": [
    "# Here be controls:\n",
    "1. Vision-Grasping cross-decoding is abolished\n",
    "2. Special turntable stops being decoded during movement\n",
    "3. grasping box keeps being decoded during movement\n",
    "4. \"control\" task performance during movement is less affected than \"active\" task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca8725",
   "metadata": {},
   "source": [
    "## Vision-Grasping cross-decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a15491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17af4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed axes = 0\n",
      "Accuracy = 0.37037037037037035\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 40.0\n",
      "Accuracy = 0.3410493827160494\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vision-grasping cross-decoding\n",
    "df_vision_trials   = df[(df['alignment']=='go cue onset') & (df['time']<0)].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_movement_trials = df[(df['alignment']=='hold onset') & (df['time']<0)].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_movement        = df[(df['alignment']=='hold onset') & (df['time']<0)].groupby(['object','trial','time'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "labels             = df_movement_trials.index.get_level_values(0)\n",
    "\n",
    "assert np.all( df_vision_trials.index.get_level_values(-1).to_numpy() == df_movement_trials.index.get_level_values(-1).to_numpy() ), 'trials not lined up between vision & movement!'\n",
    "\n",
    "flag          = False\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "splits  = list(splits) # store as list to allow it to be used many of time\n",
    "spaces2null = []\n",
    "deltamu     = []\n",
    "\n",
    "# step 1: find the set of axes for each split\n",
    "for train_,_ in splits:\n",
    "    timevary_ = df_movement.groupby(level=[0,2]).aggregate('mean')\n",
    "    static_   = timevary_.groupby(level=[0]).transform('mean')\n",
    "    \n",
    "    delta_    = timevary_.to_numpy() - static_.to_numpy()\n",
    "    \n",
    "    PCmdl = PCA()\n",
    "    PCmdl.fit(delta_)\n",
    "    \n",
    "    cvals = np.cumsum(PCmdl.explained_variance_ratio_)\n",
    "    idx=0\n",
    "    while cvals[idx]<0.95:\n",
    "        idx+=1\n",
    "    \n",
    "    spaces2null+=[PCmdl.components_[:idx,:]]\n",
    "    \n",
    "    # also get delta-mus\n",
    "    deltamu += [df_vision_trials.iloc[train_].mean() - df_movement_trials.iloc[train_].mean()]\n",
    "\n",
    "# step 2: model fitting\n",
    "for removeAxes in [False,True]:\n",
    "    correct_count = 0\n",
    "    chance_count  = 0\n",
    "    total_count   = len(labels)\n",
    "    \n",
    "    for fold,(train_,test_) in enumerate(splits):\n",
    "        trainX = df_vision_trials.iloc[train_]\n",
    "        trainy = labels[train_].to_numpy()\n",
    "        \n",
    "        testX  = df_movement_trials.iloc[test_]\n",
    "        testy  = labels[test_].to_numpy()\n",
    "        \n",
    "        if removeAxes:\n",
    "            nullSpace = null_space( np.matrix(spaces2null[fold]) )\n",
    "        else:\n",
    "            nullSpace = np.eye(neurcount)\n",
    "            \n",
    "        # projections\n",
    "        trainX = trainX.to_numpy() @ nullSpace\n",
    "        testX  = testX.to_numpy() @ nullSpace\n",
    "        \n",
    "        # now get rid of deltamu, too (this improves model generalization across epochs)\n",
    "        deltaNull = null_space(np.matrix(deltamu[fold].to_numpy() @ nullSpace))\n",
    "        \n",
    "        # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "        PCmdl = PCA(n_components=pccount)\n",
    "        trainX = PCmdl.fit_transform(trainX @ deltaNull)\n",
    "        testX  = PCmdl.transform(testX @ deltaNull)\n",
    "        \n",
    "        # now we can finally start classifying!\n",
    "        LDmdl = LDA()\n",
    "        LDmdl.fit(trainX,trainy)\n",
    "        yhat = LDmdl.predict(testX)\n",
    "        \n",
    "        correct_count += np.sum(yhat==testy)\n",
    "        chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "        \n",
    "    correct_rate   = correct_count / total_count\n",
    "    chance_rate    = chance_count / total_count\n",
    "    threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "    \n",
    "    if removeAxes:\n",
    "        print(f\"Removed axes = {np.mean( [x.shape[0] for x in spaces2null] )}\")\n",
    "    else:\n",
    "        print(f\"Removed axes = { 0 }\")\n",
    "    print(f\"Accuracy = {correct_rate}\")\n",
    "    print(f\"Chance = {chance_rate}\")\n",
    "    print(f\"Threshold = {threshold_rate}\")\n",
    "    print()\n",
    "    \n",
    "# raw cross-accuracy: roughly 40% for active, 25% for passive (for one of Moe's sessions in AIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aff279",
   "metadata": {},
   "source": [
    "## Special turntable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8f2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed axes = 0\n",
      "Accuracy = 0.8169014084507042\n",
      "Chance = 0.17610046242366476\n",
      "Threshold = 0.221305642476746\n",
      "\n",
      "Removed axes = 53.0\n",
      "Accuracy = 0.7746478873239436\n",
      "Chance = 0.17610046242366476\n",
      "Threshold = 0.221305642476746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dp.get('session')!='Zara70':\n",
    "    # special turntable control\n",
    "    df_vision_included = df[(df['alignment']=='go cue onset') & (df['time']<0) & (df['turntable']==9) & (df['context']=='active')].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='go cue onset') & (df['time']<0) & ((df['turntable']!=9) | (df['context']!='active')) ].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_movement_trials = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==9) & (df['context']=='active')].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    labels             = df_movement_trials.index.get_level_values(0)\n",
    "\n",
    "    assert np.all( df_vision_included.index.get_level_values(-1).to_numpy() == df_movement_trials.index.get_level_values(-1).to_numpy() ), 'trials not lined up between vision & movement!'\n",
    "\n",
    "    flag          = False\n",
    "    neurcount     = len(dp.get('neuronColumnNames'))\n",
    "    pccount       = 30\n",
    "    nsplits       = 5\n",
    "\n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    spaces2null = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        df_vision_cat   = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "        df_vision_agg   = df_vision_cat.groupby(level=[0,1,2]).aggregate('mean')\n",
    "        df_movement_agg = df_movement_trials.iloc[train_].groupby(level=[0]).aggregate('mean')\n",
    "        \n",
    "        viscov = np.cov(df_vision_agg.to_numpy(),rowvar=False)\n",
    "        movcov = np.cov(df_movement_agg.to_numpy(),rowvar=False)\n",
    "\n",
    "        vals,vecs = np.linalg.eigh(movcov-viscov) # sorts ascending, and we want to preferentially list the vision-preferring dimensions first\n",
    "\n",
    "        idx=0\n",
    "        while vals[idx]<-1e-6:\n",
    "            idx+=1\n",
    "\n",
    "        spaces2null+=[vecs[:,:idx].T]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removeAxes in [False,True]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_movement_trials.iloc[train_]\n",
    "            trainy = labels[train_].to_numpy()\n",
    "\n",
    "            testX  = df_movement_trials.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removeAxes:\n",
    "                nullSpace = null_space( np.matrix(spaces2null[fold]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of!\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        if removeAxes:\n",
    "            print(f\"Removed axes = {np.mean( [x.shape[0] for x in spaces2null] )}\")\n",
    "        else:\n",
    "            print(f\"Removed axes = { 0 }\")    \n",
    "        print(f\"Accuracy = {correct_rate}\")\n",
    "        print(f\"Chance = {chance_rate}\")\n",
    "        print(f\"Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483ebc6",
   "metadata": {},
   "source": [
    "## Control task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad15323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.8831168831168831\n",
      "    Chance = 0.1786280879353328\n",
      "    Threshold = 0.22227961446074423\n",
      "\n",
      "    Removed axes = 53.0\n",
      "    Accuracy = 0.8831168831168831\n",
      "    Chance = 0.1786280879353328\n",
      "    Threshold = 0.22227961446074423\n",
      "\n",
      "CONTROL\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.8285714285714286\n",
      "    Chance = 0.17857142857142858\n",
      "    Threshold = 0.22434785317583256\n",
      "\n",
      "    Removed axes = 53.0\n",
      "    Accuracy = 0.9142857142857143\n",
      "    Chance = 0.17857142857142858\n",
      "    Threshold = 0.22434785317583256\n",
      "\n",
      "PASSIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.7638888888888888\n",
      "    Chance = 0.17367412784835654\n",
      "    Threshold = 0.2183195558157913\n",
      "\n",
      "    Removed axes = 53.0\n",
      "    Accuracy = 0.6805555555555556\n",
      "    Chance = 0.17367412784835654\n",
      "    Threshold = 0.2183195558157913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control task\n",
    "# hmmmm it doesn't seem like VGG is really all that adversely affected by removal of the visual space\n",
    "# or Obs for that matter...\n",
    "# note: object & grip decoding should be the same for the \"mixed\" turntable (they are for Moe50 in any case)\n",
    "\n",
    "flag          = False\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "for context_ in ['active','control','passive']:\n",
    "    print(context_.upper())\n",
    "    df_vision_included = df[(df['alignment']=='go cue onset') & (df['time']<0) & ((df['turntable']==1) & (df['context']==context_))].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='go cue onset') & (df['time']<0) & ((df['turntable']!=1) | (df['context']!=context_)) ].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    df_classify = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==1) & (df['context']==context_)].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_cross    = df[(df['alignment']=='hold onset') & (df['time']<0) & ((df['turntable']!=1) | (df['context']!=context_))].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    labels      = df_classify.index.get_level_values(0)\n",
    "    \n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    spaces2null = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        df_vision_cat   = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "        df_vision_agg   = df_vision_cat.groupby(level=[0,1,2]).aggregate('mean')\n",
    "        df_movement_cat = pd.concat( (df_cross,df_classify.iloc[train_]) )\n",
    "        df_movement_agg = df_movement_cat.groupby(level=[0]).aggregate('mean')\n",
    "        \n",
    "        viscov = np.cov(df_vision_agg.to_numpy(),rowvar=False)\n",
    "        movcov = np.cov(df_movement_agg.to_numpy(),rowvar=False)\n",
    "\n",
    "        vals,vecs = np.linalg.eigh(movcov-viscov) # sorts ascending, and we want to preferentially list the vision-preferring dimensions first\n",
    "\n",
    "        idx=0\n",
    "        while vals[idx]<-1e-6:\n",
    "            idx+=1\n",
    "\n",
    "        spaces2null+=[vecs[:,:idx].T]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removeAxes in [False,True]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_classify.iloc[train_]\n",
    "            trainy = labels[train_].to_numpy()\n",
    "\n",
    "            testX  = df_classify.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removeAxes:\n",
    "                nullSpace = null_space( np.matrix(spaces2null[fold]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of!\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        if removeAxes:\n",
    "            print(f\"    Removed axes = {np.mean( [x.shape[0] for x in spaces2null] )}\")\n",
    "        else:\n",
    "            print(f\"    Removed axes = { 0 }\")    \n",
    "        print(f\"    Accuracy = {correct_rate}\")\n",
    "        print(f\"    Chance = {chance_rate}\")\n",
    "        print(f\"    Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fca9a",
   "metadata": {},
   "source": [
    "# full result (excluding the special turntable due to the lack of grip variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d964d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.75\n",
      "    Chance = 0.22272727272727272\n",
      "    Threshold = 0.25077915928761463\n",
      "\n",
      "    Removed axes = 22.0\n",
      "    Accuracy = 0.740909090909091\n",
      "    Chance = 0.22272727272727272\n",
      "    Threshold = 0.25077915928761463\n",
      "\n",
      "PASSIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.5576923076923077\n",
      "    Chance = 0.23079404466501238\n",
      "    Threshold = 0.2645283148467347\n",
      "\n",
      "    Removed axes = 25.0\n",
      "    Accuracy = 0.44871794871794873\n",
      "    Chance = 0.23079404466501238\n",
      "    Threshold = 0.2645283148467347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_active_trials   = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='active')].groupby(['grip','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_passive_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='passive')].groupby(['grip','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "active_labels      = df_active_trials.index.get_level_values(0)\n",
    "passive_labels     = df_passive_trials.index.get_level_values(0)\n",
    "\n",
    "flag          = False\n",
    "# axesToRemove  = 0 # don't undo your hard work!\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "for context_ in ['active','passive']:\n",
    "    print(context_.upper())\n",
    "    df_vision_included = df[(df['alignment']=='go cue onset') & (df['time']<0) & ((df['turntable']!=9) & (df['context']==context_) & (df['grip'].notna()))].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='go cue onset') & (df['time']<0) & ((df['turntable']==9) | (df['context']!=context_) | (df['grip'].isna())) ].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    if context_=='active':\n",
    "        labels = active_labels\n",
    "        df_classify = df_active_trials\n",
    "        df_movement = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='active')].groupby(['grip','trial','time'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    elif context_=='passive':\n",
    "        labels = passive_labels\n",
    "        df_classify = df_passive_trials  \n",
    "        df_movement = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='passive')].groupby(['grip','trial','time'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    spaces2null = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        timevary_ = df_movement.groupby(level=[0,2]).aggregate('mean')\n",
    "        static_   = timevary_.groupby(level=[0]).transform('mean')\n",
    "\n",
    "        delta_    = timevary_.to_numpy() - static_.to_numpy()\n",
    "\n",
    "        PCmdl = PCA()\n",
    "        PCmdl.fit(delta_)\n",
    "\n",
    "        cvals = np.cumsum(PCmdl.explained_variance_ratio_)\n",
    "        idx=0\n",
    "        while cvals[idx]<0.95:\n",
    "            idx+=1\n",
    "\n",
    "        spaces2null+=[PCmdl.components_[:idx,:]]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removeAxes in [False,True]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_classify.iloc[train_]\n",
    "            trainy = labels[train_].to_numpy()\n",
    "\n",
    "            testX  = df_classify.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removeAxes:\n",
    "                nullSpace = null_space( np.matrix(spaces2null[fold]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of!\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        if removeAxes:\n",
    "            print(f\"    Removed axes = {np.mean( [x.shape[0] for x in spaces2null] )}\")\n",
    "        else:\n",
    "            print(f\"    Removed axes = { 0 }\")    \n",
    "        print(f\"    Accuracy = {correct_rate}\")\n",
    "        print(f\"    Chance = {chance_rate}\")\n",
    "        print(f\"    Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50c891",
   "metadata": {},
   "source": [
    "# try object cross-classification across contexts, at least prove that *this* is entirely a visual effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394add48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test context = ACTIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.3402061855670103\n",
      "    Chance = 0.08362369337979093\n",
      "    Threshold = 0.09985132363803655\n",
      "\n",
      "    Removed axes = 26.4\n",
      "    Accuracy = 0.07216494845360824\n",
      "    Chance = 0.08362369337979093\n",
      "    Threshold = 0.09985132363803655\n",
      "\n",
      "Test context = PASSIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.18466898954703834\n",
      "    Chance = 0.0859106529209622\n",
      "    Threshold = 0.10245222894643412\n",
      "\n",
      "    Removed axes = 26.8\n",
      "    Accuracy = 0.06620209059233449\n",
      "    Chance = 0.0859106529209622\n",
      "    Threshold = 0.10245222894643412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_active_trials   = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['context']=='active')].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_passive_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['context']=='passive')].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "active_labels      = df_active_trials.index.get_level_values(0)\n",
    "passive_labels     = df_passive_trials.index.get_level_values(0)\n",
    "\n",
    "for context_ in ['active','passive']:\n",
    "    print(f\"Test context = {context_.upper()}\")\n",
    "    df_vision_included = df[(df['alignment']=='go cue onset') & (df['time']<0) & (df['context']==context_)].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='go cue onset') & (df['time']<0) & (df['context']!=context_)].groupby(['context','turntable','object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    if context_=='active':\n",
    "        labels = active_labels\n",
    "        df_classify = df_active_trials\n",
    "        df_cross    = df_passive_trials\n",
    "        crosslabels = passive_labels\n",
    "    elif context_=='passive':\n",
    "        labels = passive_labels\n",
    "        df_classify = df_passive_trials\n",
    "        df_cross    = df_active_trials\n",
    "        crosslabels = active_labels\n",
    "    \n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    spaces2null = []\n",
    "    deltamu     = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        df_vision_cat = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "        df_vision_agg = df_vision_cat.groupby(level=[0,1,2]).aggregate('mean')\n",
    "        PCmdl = PCA()\n",
    "        PCmdl.fit(df_vision_agg.to_numpy())\n",
    "\n",
    "        idx=0\n",
    "        vexp=PCmdl.explained_variance_ratio_[0]\n",
    "        while vexp<0.95:\n",
    "            idx+=1\n",
    "            vexp+=PCmdl.explained_variance_ratio_[idx]\n",
    "\n",
    "        spaces2null+=[PCmdl.components_[:idx,:]]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "        deltamu += [df_classify.iloc[train_].mean() - df_cross.mean()]\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removeAxes in [False,True]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_cross\n",
    "            trainy = crosslabels.to_numpy()\n",
    "\n",
    "            testX  = df_classify.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removeAxes:\n",
    "                nullSpace = null_space( np.matrix(spaces2null[fold]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of!\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        if removeAxes:\n",
    "            print(f\"    Removed axes = {np.mean( [x.shape[0] for x in spaces2null] )}\")\n",
    "        else:\n",
    "            print(f\"    Removed axes = { 0 }\")    \n",
    "        print(f\"    Accuracy = {correct_rate}\")\n",
    "        print(f\"    Chance = {chance_rate}\")\n",
    "        print(f\"    Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
