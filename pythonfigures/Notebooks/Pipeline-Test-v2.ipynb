{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff5a013",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee957798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "init=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b3464",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0833f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not init:\n",
    "    os.chdir('..')\n",
    "    init=True\n",
    "from pythonfigures.datapartition import DataPartitioner\n",
    "from pythonfigures.neuraldatabase import Query\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.linalg import subspace_angles, null_space, orth\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5647b7c",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPartitioner(session='Moe46',\n",
    "                    areas=['AIP'],\n",
    "                    aligns=['fixation','cue onset','go cue onset','movement onset','hold onset'],\n",
    "                    contexts=['active','passive','control'],\n",
    "                    groupings=['context','alignment','grip','object','turntable','time'])\n",
    "\n",
    "# TODO: try go cue onset as the alignment for the \"visual\" space (i.e., include memory signals, *really* make sure you're isolating those transients!) (also include context & turntable as separable factors in the space you try to remove, and use a 95% variance criterion like the one used in the paper instead of these weak classification criteria)\n",
    "# TODO: visualize the time-varying nature of the signals before & after removal... *somehow* (save this for after NCM)\n",
    "# TODO: active vs. passive cross-classification of OBJECT, before & after subspace removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b3b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the whole damn thing\n",
    "df = dp.readQuery(0)\n",
    "\n",
    "# convert from turntable x object ID vs. just the turntable ID\n",
    "df['turntable'] = df['turntable'] // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb0ad4",
   "metadata": {},
   "source": [
    "# Find the size of the subspace that object vision occupies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bb4b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed axes = 0\n",
      "Accuracy = 0.5848765432098766\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 1\n",
      "Accuracy = 0.5370370370370371\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 2\n",
      "Accuracy = 0.4305555555555556\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 3\n",
      "Accuracy = 0.35648148148148145\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 4\n",
      "Accuracy = 0.29475308641975306\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 5\n",
      "Accuracy = 0.23148148148148148\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 6\n",
      "Accuracy = 0.19907407407407407\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 7\n",
      "Accuracy = 0.1574074074074074\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 8\n",
      "Accuracy = 0.13734567901234568\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 9\n",
      "Accuracy = 0.09104938271604938\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Final result: remove 9 axes\n"
     ]
    }
   ],
   "source": [
    "df_vision_trials   = df[(df['alignment']=='cue onset') & (df['time']>0)].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "labels             = df_vision_trials.index.get_level_values(0)\n",
    "\n",
    "# Note: cross-decoding is simply too conservative.\n",
    "# It preserves classification of the special turntables, for instance\n",
    "# Instead, just abolish the entire space that leads to supra-chance visual classification, period\n",
    "\n",
    "flag          = False\n",
    "axesToRemove  = 0\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "splits  = list(splits) # store as list to allow it to be used many of time\n",
    "PCmdls  = []\n",
    "deltamu = []\n",
    "\n",
    "# step 1: find the set of axes for each split\n",
    "for train_,_ in splits:\n",
    "    df_vision_agg = df_vision_trials.iloc[train_].groupby(level=[0]).aggregate('mean')\n",
    "    n_components = min(df_vision_agg.shape) - 1\n",
    "    PCmdl = PCA(n_components=n_components)\n",
    "    PCmdl.fit(df_vision_agg.to_numpy())\n",
    "    PCmdls+=[PCmdl]\n",
    "    \n",
    "    # no delta-mus needed\n",
    "\n",
    "# step 2: iterate model fitting\n",
    "maxAxesToRemove = PCmdls[0].n_components_\n",
    "while not flag:\n",
    "    correct_count = 0\n",
    "    chance_count  = 0\n",
    "    total_count   = len(labels)\n",
    "    \n",
    "    for fold,(train_,test_) in enumerate(splits):\n",
    "        trainX = df_vision_trials.iloc[train_]\n",
    "        trainy = labels[train_].to_numpy()\n",
    "        \n",
    "        testX  = df_vision_trials.iloc[test_]\n",
    "        testy  = labels[test_].to_numpy()\n",
    "        \n",
    "        if axesToRemove > 0:\n",
    "            nullSpace = null_space( np.matrix(PCmdls[fold].components_[:axesToRemove,:]) )\n",
    "        else:\n",
    "            nullSpace = np.eye(neurcount)\n",
    "            \n",
    "        # projections\n",
    "        trainX = trainX.to_numpy() @ nullSpace\n",
    "        testX  = testX.to_numpy() @ nullSpace\n",
    "        \n",
    "        # no deltamu to get rid of\n",
    "        \n",
    "        # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "        PCmdl = PCA(n_components=pccount)\n",
    "        trainX = PCmdl.fit_transform(trainX)\n",
    "        testX  = PCmdl.transform(testX)\n",
    "        \n",
    "        # now we can finally start classifying!\n",
    "        LDmdl = LDA()\n",
    "        LDmdl.fit(trainX,trainy)\n",
    "        yhat = LDmdl.predict(testX)\n",
    "        \n",
    "        correct_count += np.sum(yhat==testy)\n",
    "        chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "        \n",
    "    correct_rate   = correct_count / total_count\n",
    "    chance_rate    = chance_count / total_count\n",
    "    threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "    \n",
    "    print(f\"Removed axes = {axesToRemove}\")\n",
    "    print(f\"Accuracy = {correct_rate}\")\n",
    "    print(f\"Chance = {chance_rate}\")\n",
    "    print(f\"Threshold = {threshold_rate}\")\n",
    "    print()\n",
    "    \n",
    "    if correct_rate > threshold_rate and axesToRemove<maxAxesToRemove:\n",
    "        axesToRemove+=1\n",
    "    else:\n",
    "        flag = True\n",
    "        \n",
    "print(f\"Final result: remove {axesToRemove} axes\")\n",
    "# axesToRemove = maxAxesToRemove  # forget it, remove everything (doesn't actually help much)\n",
    "# note: baseline effects in terms of turntable x context exist\n",
    "# however, we only ever classify within context, and even when we cross-classify, we remove the mean-separating subspace\n",
    "# moreover, to the extent these baseline effects persist through cue onset, these turntable effects should be, for the most part, a subset of the overall object effect (with the exception of the mixed turntable, which repeats some objects from the other turntables and thus has a turntable effect not accounted for by object ID alone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030906b1",
   "metadata": {},
   "source": [
    "# Here be controls:\n",
    "1. Vision-Grasping cross-decoding is abolished\n",
    "2. Special turntable stops being decoded during movement\n",
    "3. grasping box keeps being decoded during movement\n",
    "4. \"control\" task performance during movement is less affected than \"active\" task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca8725",
   "metadata": {},
   "source": [
    "## Vision-Grasping cross-decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17af4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed axes = 0\n",
      "Accuracy = 0.28858024691358025\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n",
      "Removed axes = 9\n",
      "Accuracy = 0.09567901234567901\n",
      "Chance = 0.09413473478984076\n",
      "Threshold = 0.10560620950962894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vision-grasping cross-decoding\n",
    "df_vision_trials   = df[(df['alignment']=='cue onset') & (df['time']>0)].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_movement_trials = df[(df['alignment']=='hold onset') & (df['time']<0)].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "labels             = df_vision_trials.index.get_level_values(0) # should be equal between \n",
    "labels_            = df_movement_trials.index.get_level_values(0)\n",
    "assert ((df_vision_trials.shape[0]>0) & (df_movement_trials.shape[0]>0)), \"no trials match test criteria\"\n",
    "assert np.all(labels==labels_), \"labels are not equal\"\n",
    "\n",
    "flag          = False\n",
    "# axesToRemove  = 0 # don't undo this!\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "splits  = list(splits) # store as list to allow it to be used many of time\n",
    "PCmdls  = []\n",
    "deltamu = []\n",
    "\n",
    "# step 1: find the set of axes for each split\n",
    "for train_,_ in splits:\n",
    "    df_vision_agg = df_vision_trials.iloc[train_].groupby(level=[0]).aggregate('mean')\n",
    "    n_components = min(df_vision_agg.shape) - 1\n",
    "    PCmdl = PCA(n_components=n_components)\n",
    "    PCmdl.fit(df_vision_agg.to_numpy())\n",
    "    PCmdls+=[PCmdl]\n",
    "    \n",
    "    # also get delta-mus\n",
    "    deltamu += [df_vision_trials.iloc[train_].mean() - df_movement_trials.iloc[train_].mean()]\n",
    "\n",
    "# step 2: model fitting\n",
    "for removedAxes in [0,axesToRemove]:\n",
    "    correct_count = 0\n",
    "    chance_count  = 0\n",
    "    total_count   = len(labels)\n",
    "    \n",
    "    for fold,(train_,test_) in enumerate(splits):\n",
    "        trainX = df_vision_trials.iloc[train_]\n",
    "        trainy = labels[train_].to_numpy()\n",
    "        \n",
    "        testX  = df_movement_trials.iloc[test_]\n",
    "        testy  = labels[test_].to_numpy()\n",
    "        \n",
    "        if removedAxes > 0:\n",
    "            nullSpace = null_space( np.matrix(PCmdls[fold].components_[:removedAxes,:]) )\n",
    "        else:\n",
    "            nullSpace = np.eye(neurcount)\n",
    "            \n",
    "        # projections\n",
    "        trainX = trainX.to_numpy() @ nullSpace\n",
    "        testX  = testX.to_numpy() @ nullSpace\n",
    "        \n",
    "        # now get rid of deltamu, too (this improves model generalization across epochs)\n",
    "        deltaNull = null_space(np.matrix(deltamu[fold].to_numpy() @ nullSpace))\n",
    "        \n",
    "        # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "        PCmdl = PCA(n_components=pccount)\n",
    "        trainX = PCmdl.fit_transform(trainX @ deltaNull)\n",
    "        testX  = PCmdl.transform(testX @ deltaNull)\n",
    "        \n",
    "        # now we can finally start classifying!\n",
    "        LDmdl = LDA()\n",
    "        LDmdl.fit(trainX,trainy)\n",
    "        yhat = LDmdl.predict(testX)\n",
    "        \n",
    "        correct_count += np.sum(yhat==testy)\n",
    "        chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "        \n",
    "    correct_rate   = correct_count / total_count\n",
    "    chance_rate    = chance_count / total_count\n",
    "    threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "    \n",
    "    print(f\"Removed axes = {removedAxes}\")\n",
    "    print(f\"Accuracy = {correct_rate}\")\n",
    "    print(f\"Chance = {chance_rate}\")\n",
    "    print(f\"Threshold = {threshold_rate}\")\n",
    "    print()\n",
    "    \n",
    "# raw cross-accuracy: roughly 40% for active, 25% for passive (for one of Moe's sessions in AIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aff279",
   "metadata": {},
   "source": [
    "## Special turntable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8f2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed axes = 0\n",
      "Accuracy = 0.8450704225352113\n",
      "Chance = 0.17610046242366476\n",
      "Threshold = 0.221305642476746\n",
      "\n",
      "Removed axes = 9\n",
      "Accuracy = 0.647887323943662\n",
      "Chance = 0.17610046242366476\n",
      "Threshold = 0.221305642476746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# special turntable control (note: more aggressive abolition of visual activity changes the drop from 80->65 to 80->40. An improvement, but still not ideal)\n",
    "df_vision_included = df[(df['alignment']=='cue onset') & (df['time']>0) & (df['turntable']==9) & (df['context']=='active')].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_vision_excluded = df[(df['alignment']=='cue onset') & (df['time']>0) & ((df['turntable']!=9) | (df['context']!='active')) ].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_movement_trials = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==9) & (df['context']=='active')].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "labels             = df_movement_trials.index.get_level_values(0)\n",
    "\n",
    "assert ((df_vision_included.shape[0]>0) & (df_vision_excluded.shape[0]>0) & (df_movement_trials.shape[0]>0)), \"no trials match test criteria\"\n",
    "\n",
    "flag          = False\n",
    "# axesToRemove  = 0 # don't undo your hard work!\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "splits  = list(splits) # store as list to allow it to be used many of time\n",
    "PCmdls  = []\n",
    "\n",
    "# step 1: find the set of axes for each split\n",
    "for train_,_ in splits:\n",
    "    df_vision_cat = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "    df_vision_agg = df_vision_cat.groupby(level=[0]).aggregate('mean')\n",
    "    n_components = min(df_vision_agg.shape) - 1\n",
    "    PCmdl = PCA(n_components=n_components)\n",
    "    PCmdl.fit(df_vision_agg.to_numpy())\n",
    "    PCmdls+=[PCmdl]\n",
    "    \n",
    "    # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "# step 2: model fitting\n",
    "for removedAxes in [0,axesToRemove]:\n",
    "    correct_count = 0\n",
    "    chance_count  = 0\n",
    "    total_count   = len(labels)\n",
    "    \n",
    "    for fold,(train_,test_) in enumerate(splits):\n",
    "        trainX = df_movement_trials.iloc[train_]\n",
    "        trainy = labels[train_].to_numpy()\n",
    "\n",
    "        testX  = df_movement_trials.iloc[test_]\n",
    "        testy  = labels[test_].to_numpy()\n",
    "\n",
    "        if removedAxes > 0:\n",
    "            nullSpace = null_space( np.matrix(PCmdls[fold].components_[:removedAxes,:]) )\n",
    "        else:\n",
    "            nullSpace = np.eye(neurcount)\n",
    "\n",
    "        # projections\n",
    "        trainX = trainX.to_numpy() @ nullSpace\n",
    "        testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "        # no deltamu to get rid of, as we're not taking measures to maximize generalizability of a cross-decoding scheme\n",
    "\n",
    "        # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "        PCmdl = PCA(n_components=pccount)\n",
    "        trainX = PCmdl.fit_transform(trainX)\n",
    "        testX  = PCmdl.transform(testX)\n",
    "\n",
    "        # now we can finally start classifying!\n",
    "        LDmdl = LDA()\n",
    "        LDmdl.fit(trainX,trainy)\n",
    "        yhat = LDmdl.predict(testX)\n",
    "\n",
    "        correct_count += np.sum(yhat==testy)\n",
    "        chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "    correct_rate   = correct_count / total_count\n",
    "    chance_rate    = chance_count / total_count\n",
    "    threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "    print(f\"Removed axes = {removedAxes}\")\n",
    "    print(f\"Accuracy = {correct_rate}\")\n",
    "    print(f\"Chance = {chance_rate}\")\n",
    "    print(f\"Threshold = {threshold_rate}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2c438",
   "metadata": {},
   "source": [
    "## Grasping Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3b05d",
   "metadata": {},
   "source": [
    "Oops sorry this was only ever run for the \"standard\" sessions in Moe's datasets, never the mirror ones! Shoot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483ebc6",
   "metadata": {},
   "source": [
    "## Control task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7cbe391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.8701298701298701\n",
      "    Chance = 0.1786280879353328\n",
      "    Threshold = 0.22227961446074423\n",
      "\n",
      "    Removed axes = 9\n",
      "    Accuracy = 0.7922077922077922\n",
      "    Chance = 0.1786280879353328\n",
      "    Threshold = 0.22227961446074423\n",
      "\n",
      "CONTROL\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.8285714285714286\n",
      "    Chance = 0.17857142857142858\n",
      "    Threshold = 0.22434785317583256\n",
      "\n",
      "    Removed axes = 9\n",
      "    Accuracy = 0.7857142857142857\n",
      "    Chance = 0.17857142857142858\n",
      "    Threshold = 0.22434785317583256\n",
      "\n",
      "PASSIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.7777777777777778\n",
      "    Chance = 0.17367412784835654\n",
      "    Threshold = 0.2183195558157913\n",
      "\n",
      "    Removed axes = 9\n",
      "    Accuracy = 0.6527777777777778\n",
      "    Chance = 0.17367412784835654\n",
      "    Threshold = 0.2183195558157913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control task\n",
    "# hmmmm it doesn't seem like VGG is really all that adversely affected by removal of the visual space\n",
    "# or Obs for that matter...\n",
    "# note: object & grip decoding should be the same for the \"mixed\" turntable (they are for Moe50 in any case)\n",
    "df_control_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==1) & (df['context']=='control')].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_active_trials   = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==1) & (df['context']=='active')].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_passive_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']==1) & (df['context']=='passive')].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "control_labels     = df_control_trials.index.get_level_values(0)\n",
    "active_labels      = df_active_trials.index.get_level_values(0)\n",
    "passive_labels     = df_passive_trials.index.get_level_values(0)\n",
    "\n",
    "assert ((df_control_trials.shape[0]>0) & (df_active_trials.shape[0]>0)), \"no trials match test criteria\"\n",
    "\n",
    "flag          = False\n",
    "# axesToRemove  = 0 # don't undo your hard work!\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "for context_ in ['active','control','passive']:\n",
    "    print(context_.upper())\n",
    "    df_vision_included = df[(df['alignment']=='cue onset') & (df['time']>0) & ((df['turntable']==1) & (df['context']==context_))].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='cue onset') & (df['time']>0) & ((df['turntable']!=1) | (df['context']!=context_)) ].groupby(['object','trial'])[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    if context_=='active':\n",
    "        labels = active_labels\n",
    "        df_classify = df_active_trials\n",
    "    elif context_=='control':\n",
    "        labels = control_labels\n",
    "        df_classify = df_control_trials\n",
    "    elif context_=='passive':\n",
    "        labels = passive_labels\n",
    "        df_classify = df_passive_trials  \n",
    "    \n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    PCmdls  = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        df_vision_cat = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "        df_vision_agg = df_vision_cat.groupby(level=[0]).aggregate('mean')\n",
    "        n_components = min(df_vision_agg.shape) - 1\n",
    "        PCmdl = PCA(n_components=n_components)\n",
    "        PCmdl.fit(df_vision_agg.to_numpy())\n",
    "        PCmdls+=[PCmdl]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removedAxes in [0,axesToRemove]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_classify.iloc[train_]\n",
    "            trainy = labels[train_].to_numpy()\n",
    "\n",
    "            testX  = df_classify.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removedAxes > 0:\n",
    "                nullSpace = null_space( np.matrix(PCmdls[fold].components_[:removedAxes,:]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of, as we're not taking measures to maximize generalizability of a cross-decoding scheme\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        print(f\"    Removed axes = {removedAxes}\")\n",
    "        print(f\"    Accuracy = {correct_rate}\")\n",
    "        print(f\"    Chance = {chance_rate}\")\n",
    "        print(f\"    Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fca9a",
   "metadata": {},
   "source": [
    "# full result (excluding the special turntable due to the lack of grip variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa04c855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.75\n",
      "    Chance = 0.22272727272727272\n",
      "    Threshold = 0.25077915928761463\n",
      "\n",
      "    Removed axes = 9\n",
      "    Accuracy = 0.6863636363636364\n",
      "    Chance = 0.22272727272727272\n",
      "    Threshold = 0.25077915928761463\n",
      "\n",
      "PASSIVE\n",
      "    Removed axes = 0\n",
      "    Accuracy = 0.5576923076923077\n",
      "    Chance = 0.23079404466501238\n",
      "    Threshold = 0.2645283148467347\n",
      "\n",
      "    Removed axes = 9\n",
      "    Accuracy = 0.4551282051282051\n",
      "    Chance = 0.23079404466501238\n",
      "    Threshold = 0.2645283148467347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control task\n",
    "# hmmmm it doesn't seem like VGG is really all that adversely affected by removal of the visual space\n",
    "# or Obs for that matter...\n",
    "# note: object & grip decoding should be the same for the \"mixed\" turntable (they are for Moe50 in any case)\n",
    "# note: had to turn off group key sorting here, as for the first time we're decoding *grip* during movement but *object* during vision, which means the trial indices get scrambled if we preserve typical sorting behavior\n",
    "df_active_trials   = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='active')].groupby(['grip','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_passive_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['turntable']!=9) & (df['context']=='passive')].groupby(['grip','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "active_labels      = df_active_trials.index.get_level_values(0)\n",
    "passive_labels     = df_passive_trials.index.get_level_values(0)\n",
    "\n",
    "assert ((df_control_trials.shape[0]>0) & (df_active_trials.shape[0]>0)), \"no trials match test criteria\"\n",
    "\n",
    "flag          = False\n",
    "# axesToRemove  = 0 # don't undo your hard work!\n",
    "neurcount     = len(dp.get('neuronColumnNames'))\n",
    "pccount       = 30\n",
    "nsplits       = 5\n",
    "\n",
    "for context_ in ['active','passive']:\n",
    "    print(context_.upper())\n",
    "    df_vision_included = df[(df['alignment']=='cue onset') & (df['time']>0) & ((df['turntable']!=9) & (df['context']==context_) & (df['grip'].notna()))].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    df_vision_excluded = df[(df['alignment']=='cue onset') & (df['time']>0) & ((df['turntable']==9) | (df['context']!=context_) | (df['grip'].isna())) ].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "    \n",
    "    if context_=='active':\n",
    "        labels = active_labels\n",
    "        df_classify = df_active_trials\n",
    "    elif context_=='passive':\n",
    "        labels = passive_labels\n",
    "        df_classify = df_passive_trials  \n",
    "    \n",
    "    skf     = StratifiedKFold(n_splits=nsplits,shuffle=False)\n",
    "    splits  = skf.split(np.zeros(len(labels)),labels)\n",
    "    splits  = list(splits) # store as list to allow it to be used many of time\n",
    "    PCmdls  = []\n",
    "\n",
    "    # step 1: find the set of axes for each split\n",
    "    for train_,_ in splits:\n",
    "        df_vision_cat = pd.concat( (df_vision_excluded,df_vision_included.iloc[train_]) )\n",
    "        df_vision_agg = df_vision_cat.groupby(level=[0]).aggregate('mean')\n",
    "        n_components = min(df_vision_agg.shape) - 1\n",
    "        PCmdl = PCA(n_components=n_components)\n",
    "        PCmdl.fit(df_vision_agg.to_numpy())\n",
    "        PCmdls+=[PCmdl]\n",
    "\n",
    "        # no need for deltamu, as we're not doing cross-decoding!\n",
    "\n",
    "    # step 2: model fitting\n",
    "    for removedAxes in [0,axesToRemove]:\n",
    "        correct_count = 0\n",
    "        chance_count  = 0\n",
    "        total_count   = len(labels)\n",
    "\n",
    "        for fold,(train_,test_) in enumerate(splits):\n",
    "            trainX = df_classify.iloc[train_]\n",
    "            trainy = labels[train_].to_numpy()\n",
    "\n",
    "            testX  = df_classify.iloc[test_]\n",
    "            testy  = labels[test_].to_numpy()\n",
    "\n",
    "            if removedAxes > 0:\n",
    "                nullSpace = null_space( np.matrix(PCmdls[fold].components_[:removedAxes,:]) )\n",
    "            else:\n",
    "                nullSpace = np.eye(neurcount)\n",
    "\n",
    "            # projections\n",
    "            trainX = trainX.to_numpy() @ nullSpace\n",
    "            testX  = testX.to_numpy() @ nullSpace\n",
    "\n",
    "            # no deltamu to get rid of, as we're not taking measures to maximize generalizability of a cross-decoding scheme\n",
    "\n",
    "            # now fit a PC model in the remaining space to make sure LDA input is well-conditioned (this improves cross-validated performance)\n",
    "            PCmdl = PCA(n_components=pccount)\n",
    "            trainX = PCmdl.fit_transform(trainX)\n",
    "            testX  = PCmdl.transform(testX)\n",
    "\n",
    "            # now we can finally start classifying!\n",
    "            LDmdl = LDA()\n",
    "            LDmdl.fit(trainX,trainy)\n",
    "            yhat = LDmdl.predict(testX)\n",
    "\n",
    "            correct_count += np.sum(yhat==testy)\n",
    "            chance_count  += len(testy) * max(LDmdl.priors_)\n",
    "\n",
    "        correct_rate   = correct_count / total_count\n",
    "        chance_rate    = chance_count / total_count\n",
    "        threshold_rate = chance_rate + np.sqrt( chance_rate*(1-chance_rate)/total_count ) # chance + 1 SE\n",
    "\n",
    "        print(f\"    Removed axes = {removedAxes}\")\n",
    "        print(f\"    Accuracy = {correct_rate}\")\n",
    "        print(f\"    Chance = {chance_rate}\")\n",
    "        print(f\"    Threshold = {threshold_rate}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50c891",
   "metadata": {},
   "source": [
    "# try object cross-classification across contexts, at least prove that *this* is entirely a visual effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec3cd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3402061855670103\n"
     ]
    }
   ],
   "source": [
    "df_active_trials   = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['context']=='active')].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "df_passive_trials  = df[(df['alignment']=='hold onset') & (df['time']<0) & (df['context']=='passive')].groupby(['object','trial'],sort=False)[dp.get('neuronColumnNames')].aggregate('mean')\n",
    "active_labels      = df_active_trials.index.get_level_values(0)\n",
    "passive_labels     = df_passive_trials.index.get_level_values(0)\n",
    "\n",
    "PCmdl = PCA(n_components=30)\n",
    "passiveX   = PCmdl.fit_transform(df_passive_trials.to_numpy())\n",
    "activeX    = PCmdl.transform(df_active_trials.to_numpy())\n",
    "\n",
    "LDmdl = LDA()\n",
    "LDmdl.fit(passiveX,passive_labels.to_numpy())\n",
    "yhat = LDmdl.predict(activeX)\n",
    "\n",
    "print( np.mean(yhat == active_labels.to_numpy()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c901e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
